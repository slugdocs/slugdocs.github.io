{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SlugDocs A collaborative knowledge-sharing site and course archive for students at the University of California, Santa Cruz Mission Statement The purpose of this site is to capture and archive the collective knowledge of students in UCSC's various degree programs. This is not a platform like Chegg or Slader. This platform is designed to facilitate and record the construction of knowledge and the cultivation of an inclusive, supportive academic community. It is a source for peer-to-peer understanding, accessible notes, practice problems, and re-imaginations of textbook content in a friendlier way. In order to maintain the integrity of the site, all submissions are to align with the following guidelines: Cite your sources! Especially for worked problems. If you got a helpful hint from StackOverflow, make note of it. If you copied or paraphrased an explanation directly or indirectly from someone else, make note of it. Transparency is the foundation of integrity. When submitting notes, please label the lecturer , the class , and the quarter . Don't cheat! This is a reference , a conversation about learning and how it plays out with respect to various course concepts. not a solutions manual. Posts that link to solutions or source code, or anything else that's questionable will be removed. Take responsibility for the environment that we are all a part of. Please be thoughtful with your language when making submissions. We co-exist on a diverse campus with a delightful variety of different human groups with their respective sensitivities. As a general rule, be respectful. This is not a place to bash professors, flex your knowledge, or demean others. Take common decency into consideration, especially if your work contains human examples. No lab/programming-assignment specific content will be hosted on this site. Labs and programming assignments are designed for exposure to course concepts . Labs and PA's are designed to be worked through as a concrete application of fundamental course knowledged. They are not to be 'learned' or 'hacked'. Obey all University policy while making submissions. As a reference, please point your browser here . Submission Guidelines These are meant as a short reference. Please see the Submissions Guidelines page for more information on how to contribute. We want to hear your voice, and your understanding, with the emphasis on the you . Be sincere. We are open to half-finished content, and questions, though it's best to document your knowledge as it stands. Let the process of drafting submissions be a part of the learning process. Feel free to speak in casual, respectful language, using precision where necessary and fitting. Please do not submit content that is clearly (or subtly) designed to Solve homework problems 'Hack' a class Encourage any type of unethical behavior As always, read the Community Guidelines and the full Submissions Guidelines before submission About this Site This site was built using the MKDocs framework for static site creation, and the absolutely gorgeous Material Theme for MKDocs. All LaTeX equation embedding has been handled by KaTeX , and comments are enabled through Disqus . If you'd like to learn more about what's going on under the hood, direct yourself to the GitHub repository for this site, or send us an email.","title":"Home"},{"location":"#slugdocs","text":"A collaborative knowledge-sharing site and course archive for students at the University of California, Santa Cruz","title":"SlugDocs"},{"location":"#mission-statement","text":"The purpose of this site is to capture and archive the collective knowledge of students in UCSC's various degree programs. This is not a platform like Chegg or Slader. This platform is designed to facilitate and record the construction of knowledge and the cultivation of an inclusive, supportive academic community. It is a source for peer-to-peer understanding, accessible notes, practice problems, and re-imaginations of textbook content in a friendlier way. In order to maintain the integrity of the site, all submissions are to align with the following guidelines: Cite your sources! Especially for worked problems. If you got a helpful hint from StackOverflow, make note of it. If you copied or paraphrased an explanation directly or indirectly from someone else, make note of it. Transparency is the foundation of integrity. When submitting notes, please label the lecturer , the class , and the quarter . Don't cheat! This is a reference , a conversation about learning and how it plays out with respect to various course concepts. not a solutions manual. Posts that link to solutions or source code, or anything else that's questionable will be removed. Take responsibility for the environment that we are all a part of. Please be thoughtful with your language when making submissions. We co-exist on a diverse campus with a delightful variety of different human groups with their respective sensitivities. As a general rule, be respectful. This is not a place to bash professors, flex your knowledge, or demean others. Take common decency into consideration, especially if your work contains human examples. No lab/programming-assignment specific content will be hosted on this site. Labs and programming assignments are designed for exposure to course concepts . Labs and PA's are designed to be worked through as a concrete application of fundamental course knowledged. They are not to be 'learned' or 'hacked'. Obey all University policy while making submissions. As a reference, please point your browser here .","title":"Mission Statement"},{"location":"#submission-guidelines","text":"These are meant as a short reference. Please see the Submissions Guidelines page for more information on how to contribute. We want to hear your voice, and your understanding, with the emphasis on the you . Be sincere. We are open to half-finished content, and questions, though it's best to document your knowledge as it stands. Let the process of drafting submissions be a part of the learning process. Feel free to speak in casual, respectful language, using precision where necessary and fitting. Please do not submit content that is clearly (or subtly) designed to Solve homework problems 'Hack' a class Encourage any type of unethical behavior As always, read the Community Guidelines and the full Submissions Guidelines before submission","title":"Submission Guidelines"},{"location":"#about-this-site","text":"This site was built using the MKDocs framework for static site creation, and the absolutely gorgeous Material Theme for MKDocs. All LaTeX equation embedding has been handled by KaTeX , and comments are enabled through Disqus . If you'd like to learn more about what's going on under the hood, direct yourself to the GitHub repository for this site, or send us an email.","title":"About this Site"},{"location":"community-guidelines/","text":"Community Guidelines","title":"Community Guidelines"},{"location":"community-guidelines/#community-guidelines","text":"","title":"Community Guidelines"},{"location":"feedback/","text":"Loading\u2026","title":"Feedback"},{"location":"howto/","text":"How to Use This Site","title":"How to Navigate This Site"},{"location":"howto/#how-to-use-this-site","text":"","title":"How to Use This Site"},{"location":"submissions/","text":"Submissions We would love to incorporate your work into SlugDocs. This site will only grow richer with more voices and perspectives. However, there's some ethical and institutional constraints to the type of material that can be hosted on this site. If you're concerned about the content of your submission, don't fret. Submit what you have, and the editors will be sure to let you know if the submission meets the site standards and community guidelines. We want your content here , and we're more than happy to work with you throughout the submissions process. The Basics Please refrain from submitting anything that falls into one of the following categories: Worked homework problems Worked midterm/final review problems Exam questions, or material that emulates exam content Lab assignments or programming assignments Uncited lecture notes Anything that takes a \"professor-oriented\" approach Example: an article that contains something like \"Typically, Dr. WhoEver likes to ask questions like this... if you're faced with something like this, do X, Y, Z...\" Plagiarized work Any capstone or final-project-type content Submitting Content We are open to submissions of all sorts. Every submission will be part of a conversation of how the content fits into the larger SlugDocs ecosystem. So, when making submissions, send a brief description with the following information: What did you write about? Just a 10,000 foot view here. Example: This article covers the insolvability of quintic polynomials. It follows a proof by X, and ends with a discussion of the implications for making better toast Where should we put it? Where did you imagine this content would live? What should the page title be? Department and course are the most applicable here. If it builds on an existing section, we'll reach out to you to start a larger discussion about how it fits in, and we can work out the details from there. If it's a novel section, such as content for a new class, we'll probably ask some more questions, and may request a bit more material (just enough to get the skeleton of a course homepage built, etc.) How do you want us to edit it? If it's a take-it or leave-it sort of situation, that's fine! If you're coming to us with a draft, let us know! We accept all sorts of work for submissions. Even if you aren't sure how it could fit in or where it could go, there's a good chance that it has a home here (provided it meets the integrity guidelines, of course). Also, do you want to be directly associated, or left anonymous? Under a pseudonym? Let us know. Anything else we might need to know For example, if you're concerned about the integrity of a submission, or not so certain if what you wrote is wholly accurate, let us know! We'll work it out. How do I submit work? There's a number of ways that you can submit, depending on the workflow that feels the most comfortable. All of the content on this site is written in Markdown, which is kinda like code, but not really. This being said, Markdown enables us to do all sorts of cool things (like the embedded LaTeX equations, links, and other neat widgets), and it also contstrains us a little bit (all of the documents look pretty homogeneous, you'll have to send any images as their own files, etc.) If you're totally new to Markdown, consult the Workflow page for more information. E-Mail Submissions (Markdown) Contact us at slugdocs.submissions@gmail.com . Please include: A description of your submission, as described above All Markdown files Any assets used in the Markdown files (photos, gifs, etc) Container Content, Errors, Errata If you'd like to submit content that updates a page that isn't course-specific (like this one), point out an error in an existing article, or anything else that doesn't quite fit anywhere else, send us an email or fill out a feedback form. E-Mail Submissions (Not Markdown) We also accept work that isn't formatted in Markdown. Send any and all content to slugdocs.submissions@gmail.com . However, if you aren't submitting work that's already in Markdown, please make clear what the section headers will be, how you'd like the content to look on the page, and any other structural details that may not be clear from your submission format. We will contact you if we're uncertain about anything. GitHub Submissions Clone the SlugDocs-submissions repository. Follow the instructions on the README. When you're ready for feedback, open a pull request. Old School Submissions If you see any of us around on campus, feel free to approach and strike up a conversation. If you own a carrier pigeon, we're about it. Submission Workflows & Writing Process See the Workflow page for details.","title":"Submissions"},{"location":"submissions/#submissions","text":"We would love to incorporate your work into SlugDocs. This site will only grow richer with more voices and perspectives. However, there's some ethical and institutional constraints to the type of material that can be hosted on this site. If you're concerned about the content of your submission, don't fret. Submit what you have, and the editors will be sure to let you know if the submission meets the site standards and community guidelines. We want your content here , and we're more than happy to work with you throughout the submissions process.","title":"Submissions"},{"location":"submissions/#the-basics","text":"Please refrain from submitting anything that falls into one of the following categories: Worked homework problems Worked midterm/final review problems Exam questions, or material that emulates exam content Lab assignments or programming assignments Uncited lecture notes Anything that takes a \"professor-oriented\" approach Example: an article that contains something like \"Typically, Dr. WhoEver likes to ask questions like this... if you're faced with something like this, do X, Y, Z...\" Plagiarized work Any capstone or final-project-type content","title":"The Basics"},{"location":"submissions/#submitting-content","text":"We are open to submissions of all sorts. Every submission will be part of a conversation of how the content fits into the larger SlugDocs ecosystem. So, when making submissions, send a brief description with the following information: What did you write about? Just a 10,000 foot view here. Example: This article covers the insolvability of quintic polynomials. It follows a proof by X, and ends with a discussion of the implications for making better toast Where should we put it? Where did you imagine this content would live? What should the page title be? Department and course are the most applicable here. If it builds on an existing section, we'll reach out to you to start a larger discussion about how it fits in, and we can work out the details from there. If it's a novel section, such as content for a new class, we'll probably ask some more questions, and may request a bit more material (just enough to get the skeleton of a course homepage built, etc.) How do you want us to edit it? If it's a take-it or leave-it sort of situation, that's fine! If you're coming to us with a draft, let us know! We accept all sorts of work for submissions. Even if you aren't sure how it could fit in or where it could go, there's a good chance that it has a home here (provided it meets the integrity guidelines, of course). Also, do you want to be directly associated, or left anonymous? Under a pseudonym? Let us know. Anything else we might need to know For example, if you're concerned about the integrity of a submission, or not so certain if what you wrote is wholly accurate, let us know! We'll work it out.","title":"Submitting Content"},{"location":"submissions/#how-do-i-submit-work","text":"There's a number of ways that you can submit, depending on the workflow that feels the most comfortable. All of the content on this site is written in Markdown, which is kinda like code, but not really. This being said, Markdown enables us to do all sorts of cool things (like the embedded LaTeX equations, links, and other neat widgets), and it also contstrains us a little bit (all of the documents look pretty homogeneous, you'll have to send any images as their own files, etc.) If you're totally new to Markdown, consult the Workflow page for more information.","title":"How do I submit work?"},{"location":"submissions/#e-mail-submissions-markdown","text":"Contact us at slugdocs.submissions@gmail.com . Please include: A description of your submission, as described above All Markdown files Any assets used in the Markdown files (photos, gifs, etc)","title":"E-Mail Submissions (Markdown)"},{"location":"submissions/#container-content-errors-errata","text":"If you'd like to submit content that updates a page that isn't course-specific (like this one), point out an error in an existing article, or anything else that doesn't quite fit anywhere else, send us an email or fill out a feedback form.","title":"Container Content, Errors, Errata"},{"location":"submissions/#e-mail-submissions-not-markdown","text":"We also accept work that isn't formatted in Markdown. Send any and all content to slugdocs.submissions@gmail.com . However, if you aren't submitting work that's already in Markdown, please make clear what the section headers will be, how you'd like the content to look on the page, and any other structural details that may not be clear from your submission format. We will contact you if we're uncertain about anything.","title":"E-Mail Submissions (Not Markdown)"},{"location":"submissions/#github-submissions","text":"Clone the SlugDocs-submissions repository. Follow the instructions on the README. When you're ready for feedback, open a pull request.","title":"GitHub Submissions"},{"location":"submissions/#old-school-submissions","text":"If you see any of us around on campus, feel free to approach and strike up a conversation. If you own a carrier pigeon, we're about it.","title":"Old School Submissions"},{"location":"submissions/#submission-workflows-writing-process","text":"See the Workflow page for details.","title":"Submission Workflows &amp; Writing Process"},{"location":"workflow/","text":"","title":"Workflow"},{"location":"CSE/","text":"The Homepage for CSE-related materal","title":"Home"},{"location":"CSE/#the-homepage-for-cse-related-materal","text":"","title":"The Homepage for CSE-related materal"},{"location":"CSE/asym-funcs/","text":"Asymptotic Growth of Functions [[ ADD SOME PRELIM NOTES HERE ]] For the purposes of algorithmic analysis, we're particularly concerned with how an algorithm performs when the inputs get really, really large. Two functions that sort a 10-element array at around the same speed may behave very, very differently when that 10 element array has 100, 10000, or 10000000000000 elements. As such, to explore, analyze, and classify the runtimes of various algorithms, we use (and abuse) asymptotic notation . At the heart of this exploration, we have five sets : $\\Theta$, $O$, $\\Omega$, $o$, and $\\omega$. These collections allow us to compare functions relative to their asymptotic growth rates. In other words, these sets group functions based on how they behave for sufficiently large inputs, relative to another function . [[ ADD FIGURE ]] For now, think of expressions like $f(n) = \\Omega(g(n))$ as saying 'the function $f$ is related to the function $g$ by a relationship defined by $\\Omega$' . Some Things There's a couple things that we need to make clear from the onset. First, we restrict ourself to analyzing asymptotically nonnegative and asymptotically positive functions. All this means is that our functions stay positive and/or nonzero as they approach infinity. Second, the constructs $\\Theta, O, \\Omega, o, \\omega$ are sets , but we follow this strange convention of writing $f(n) = O(g(n))$ or $f(n) = \\Theta(g(n))$ to mean that $f(n)$ is in the set $O(g(n))$ or $\\Theta(g(n))$. Typically, we would use the set inclusion notation \"$\\in$\" but apparently that's out of style. (Too archaic? No ASCII symbol? I dunno.) Big O Let's start with $O$ (big-oh). $O(g(n))$ is a set, a collection of all the functions $f(n)$ such that $f(n)$ is less than or equal to a constant multiple of $g(n)$, when $n$ is sufficiently large. In other words, $O(g(n))$ contains all functions bounded above by some factor of $g(n)$. Visually, this means that [[ INSERT FIGURE ]] As you can see, we're really only concerned with the behavior of functions relative to each other after a certain 'sticking point'. In the graph above, $f(n)$ does some wiggling around before it settles down to be bounded above by $c\\cdot g(n)$, and that's fine by us. On an infinite scale, it's still eventually bounded above by some multiple of $g$, and that's all that we really care about. To be formal, we can define the set $O$ explicitly, as follows In symbols: $$O(g(n)) = \\bigg\\lbrace f(n)\\,\\bigg|\\, \\exists c, n_0 \\,\\text{such that}\\, \\forall n \\geq n_0, \\,\\, 0\\leq f(n) \\leq cg(n)\\bigg\\rbrace$$ In words: The set $O(n)$ is defined as all functions $f$ such that for a choice of $c$, and a constant $n_0$, $f(n)$ is bounded above by $c\\cdot g(n)$. This states that $O(g(n))$ is the set of all functions $f(n)$ such that $f(n)$ is bounded above by a constant multiple of $g(n)$ for some constant $c$ and a 'sticking point' $n_0$. This formal definition really just re-iterates what we can see in the figure. $O(g(n))$ contains anything that can be bounded above by $g(n)$ or some constant multiple of $g(n)$, provided that we let $n$ get big enough. And, the formal definition enables us to \"prove\" that something is in $O(g(n))$, which is one of our primary goals for any of these asymptotic collections. Let's take a look at some examples, starting with a very simple one. Let $f(n) = n$ and $g(n) = n^2$. Show that $f(n) = O(g(n))$. How do we show this? Well, we know by the definition that we need to show that $0\\leq f(n)\\leq cg(n)$ for some constant $c$, and all $n \\geq n_0$. When $n=1$, we can see that $n = n^2$. After that point, $n^2 > n$. So, if we let $c=1$ and $n_0 = 1$, we find that $0\\leq f(n)\\leq cg(n)$ for all $n \\geq n_0$. As $1$ is a nonzero constant, the definition of $O$ is satisfied. And that's it! For relatively simple functions, all we need to do is find the coefficients. It's an example of constructive proof . Let's take a look at another. Prove or disprove: $3n^2+2 = O(n^2)$ > Prove or disprove: $3n^2+2 = O(n^2)$ > > In this example, the abstracted functions $f(n)$ and $g(n)$ we were working with have been replaced by their literals, $3n^2+2$ and $n^2$. > >At first, it may seem unreasonable to say that this assertion is true. How could $3n^2+2$ be bounded above by $n^2$? The secret is in the coefficients. Remember that $O(n^2)$ means that we compare functions to any constant multiple of $n^2$, not just the function itself. > > For the sake of example, look back to the definition of $O(g(n))$. As you'll see, we're entitled to **any** coefficient we want. So, it would not be \"out of bounds\" to let $c = 10,000$, for example. Clearly, $3n^2+2 \\leq 10000\\cdot n^2$ for all $n$, which proves that $3n^2+2 = O(n^2)$. However, it's helpful to illustrate a more realistic search for constants. > Let's start with what we're trying to show: > > $$0\\leq 3n^2+2 \\leq c\\cdot n^2$$ > > With a bit of re-arrangement, we get > > $$0\\leq 3 + \\frac{2}{n^2}\\leq c$$ > Now, recall that we have more or less full control over the values of $n_0$ and $c$, so long as they are constant and nonzero. For example, if we fix $n_0 = 1$, we can pretty readily determine the required coefficient to satisfy the $O(n^2)$ definition. Holding $n_0 = 1$: > > $$0\\leq 3+2 = 5 \\leq c$$ > This tells us that for all $n \\geq n_0 = 1$, and for the constant $c=5$, > > $$ 0\\leq 3n^2+2 \\leq n^2$$ > Which is precisely the definition of $3n^2+2 = O(n^2)$, completing the proof. As shown in the examples, we can prove that $f(n)=O(g(n))$ by finding a precise set of coefficients that satisfies the relation. We'll see in a little bit how we can use limits to prove these relationships without needing to find a specific example. In Summary : - Big Oh, $O(g(n))$, defines a set of functions bounded above by constant multiples of the function $g(n)$. - One way to prove that $f(n) =O(g(n))$ is to find an explicit set of coefficients that satisfy the definition: $$0\\leq f(n)\\leq c\\cdot g(n) \\quad \\forall n\\geq n_0$$ Big Omega As we just saw, $O(g(n))$ is a collection of functions bounded above by constant multiples of $g(n)$. We can explore other types of asymptotic relationships, too. Big Omega, $\\Omega(g(n))$, is another collection, this time gathering all the functions that are bounded below by constant multiples of $g(n)$. This isn't all too different from what we just saw with $O(g(n))$, except the set is just describing a different relationship. The formal definition for $\\Omega(g(n))$ is the following: $$\\Omega(g(n)) = \\bigg\\lbrace\\,f(n)\\,\\bigg|\\, \\exists c, n_0 \\,\\,\\text{such that}\\,\\, 0\\leq c\\cdot g(n) \\leq f(n) \\,\\,\\text{for all}\\,\\, n\\geq n_0\\bigg\\rbrace$$ Which, in English, means that $\\Omega(g(n))$ is a collection of all functions that are bounded below by a constant multiple of $g(n)$, for $n$ sufficiently large. (Precisely speaking, for $n$ greater than some constant $n_0$.) As before, we can visualize $\\Omega(g(n))$ with respect to some arbitrary function $f(n)$. [[ INSERT FIGURE ]] As most of this should be a re-interpretation of the concepts we covered with $O(g(n))$, let's dive right into some exercises. Provide an example here TODO Do another example here, to show that something can be both $\\Omega$ and $O(g(n))$. What the previous example shows us is that $\\Omega(g(n))$ and $O(g(n))$ aren't mutually exclusive. Interesting, right? The key here is, yet again, the way we are allowed to use constants while making asymptotic comparisons. For all practical purposes, constants are more or less irrelevant when making comparisons with respect to $\\Omega$ or $\\Theta$ of some function $g$. We'll formalize this intuition later, but if you think about what's going on, it makes a lot of sense. Even scalar multiples such as $10000\\cdot n^2$ don't make much of a difference at an infinite scale. However, when a function $f$ is related to $g$ by both $f(n)=O(g(n))$ and $f(n)=\\Omega(g(n))$, we can say even more, as we'll see in the next section. In Summary - Like $O(g(n))$, $\\Omega(g(n))$ characterizes functions that have an asymptotic relationship to $g(n)$. - Unlike $O(g(n))$, $\\Omega(g(n))$ collects all the functions that are bounded below by a constant multiple of $g(n)$. - For now, we stick to finding explicit coefficients to prove $f(n)=\\Omega(g(n))$, just as we found coefficients for $O(g(n))$. $\\Theta$ In the previous section, we saw that a function $f(n)$ can \"float\" between two multiples of another function $g(n)$, thereby satisfying $f=O(g(n))$ and $f=\\Omega(g(n))$. If we were to draw this on a graph for some aribitrary functions $f$ and $g$, it would look something like this: [[ INSERT FIGURE ]] As it turns out, this isn't all that uncommon. In fact, we have a special notation for these types of situations. We say that $f(n) = \\Theta(g(n))$ (Theta of g of n) when $f(n)$ is smooshed between two constant multiples of $g(n)$, for all $n$ greater than some constant $n_0$. In set-builder notation, this means: $$\\Theta(g(n)) = \\bigg\\lbrace\\,f(n)\\,\\bigg|\\,\\exists c_0, c_1, n_0 \\,\\text{such that}\\,\\, 0\\leq c_0g(n)\\leq f(n)\\leq c_1g(n)\\,\\text{for all}\\,\\, n\\geq n_0\\bigg\\rbrace$$ Notice that if $f(n)=O(g(n))$, and $f(n)=\\Omega(g(n))$, then $f(n)=\\Theta(g(n))$, by default. To see why, consider that - $f(n)=O(g(n))$ implies that for some constant $c_1$ and another constant $n_0$, $0\\leq f(n)\\leq c_1g(n)$ for $n\\geq n_0$. - $f(n)=\\Omega(g(n))$ implies that for some constant $c_0$ and another constant $n_1$, $0\\leq c_0g(n)\\leq f(n)$ for $n\\geq n_1$. - For all $n$ greater than $n_0$ or $n_1$ (whichever is bigger), both of the statements above will be true at the same time. - In symbols, this means that for $n\\geq max(n_0, n_1)$, $$0\\leq c_0g(n)\\leq f(n)\\leq c_1g(n)$$ Which is just what we wanted! $\\Theta(g(n))$ defines what is called a tight asymptotic bound. To prove $f(n)=\\Theta(g(n))$ for functions $f$ and $g$, we can do the coefficient hunting that we did before. Only this time, we need to find two coefficients. Personally, I find it a bit easier to solve $f(n)=\\Omega(g(n))$ and $f(n)=O(g(n))$ separately, and then combine the coefficients for $n$ greater than the max of the sticking point for each function. Here's an example of that idea at work. Example with $\\Omega$ and $O$ to prove $\\Theta$ TODO Internal/Dev note: play up the sticking point bit a little bit more. I think that it's instructive and intuitive. In Summary - $\\Theta, O, \\Omega$ all relate growth rates - $O(g(n))$ characterizes all functions bounded above by a factor of $g(n)$. - $\\Omega(g(n))$ characterizes all functions bounded below by a factor of $g(n)$. - $\\Theta(g(n))$ characterizes functions smooshed in between two factors of $g(n)$ - $\\bigg(f(n)=\\Omega(g(n))\\bigg) + \\bigg(f(n)=O(g(n))\\bigg) = \\bigg(f(n) = \\Theta(g(n))\\bigg)$ - For now, we're still stuck with finding coefficients to prove that this is true. Limit Definitions Using limits to define $\\Theta, \\Omega$, and $O$ is particularly convenient. Really, we're investigating the limiting behavior (what happens at $\\infty$) of functions relative to each other, so the limit is almost a natural consequence. Take the statement $f(n)=O(g(n))$, for example. The definition of $O(g(n))$ tells us that this means $$0\\leq f(n)\\leq c\\cdot g(n)$$ For a sufficiently large $n$. However, if we divide each side by $g(n)$, we get $$0\\leq \\frac{f(n)}{g(n)} \\leq c \\qquad (1)$$ As $c$ is a constant, this inequality is equivalent to the limit $$\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = L \\quad \\text{where}\\quad 0\\leq L < \\infty \\qquad (2)$$ The limit can't be infinite, because we're locked down by the constant on the far right of the inequality. The limit, however, can be zero. If this is confusing, remember that the inequality $(1)$ must be true for all $n$. So, even as $n$ becomes infinitely large, $\\frac{f(n)}{g(n)}$ must be some finite number. We can define a similar limit for $\\Omega(g(n))$. $f(n) = \\Omega(g(n))$ implies that for sufficiently large $n$, and a constant $c$, $$0\\leq c\\cdot g(n) \\leq f(n)$$ Re-arranging as before, we can see that $$0< c\\leq \\frac{f(n)}{g(n)}$$ Which means, that as the inputs become arbitrarily large, $\\frac{f(n)}{g(n)}$ needs to stay above a certain constant threshold. Translating that inequality into a limit, we find $$\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = L \\quad \\text{where} \\quad 0 < L\\leq \\infty$$ These limits are awesome for two reasons: first, they're a bit more intuitive for comparing the relative behavior of functions as the inputs approach arbitrarily large values. Second, it makes solving complex asymptotic relationships really really easy. Here's a couple of examples to gnaw on. [[ INSERT EXAMPLES ]] $\\Theta(g(n))$ has a natural limit definition, too. The definition of $f(n) = \\Theta(g(n))$ implies that for nonzero constants $c_0, c_1$, and for $n$ beyond the sticking point $n_0$, $$0\\leq c_0g(n) \\leq f(n) \\leq c_1g(n)$$ Dividing through by $g(n)$ gives $$0 < c_0 \\leq \\frac{f(n)}{g(n)} \\leq c_1$$ Which, as a limit, implies that $$\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = L \\quad \\text{where} \\quad 0 < L < \\infty$$ Just as $\\Omega + O = \\Theta$ when we were playing around with constants, we can see that when the limiting behavior of $\\frac{f(n)}{g(n)}$ satisfies the limit definitions for both $O$ and $\\Omega$, it will naturally satisfy the limit definition for $\\Theta$. Using these limits, we can start to develop a more sophisticated insight into how functions behave relative to each other. Littles We have two more sets to talk about before we can wrap up our exploration of asymptotic function analysis. For the purposes of comparing functions and analyzing algorithms, it can be particularly useful to classify and collection functions that aren't within a constant multiple of the function that we're comparing them to. This really isn't all that different from the things that we were doing before: the bounds just become a bit more strict. We have two collections that encode this type of information: $o$ (little-oh), which is an analog to $O$, and $\\omega$ (little-omega), which is an analog to $\\Omega$. The definitions and explanations that follow will look similar, but there's some important, subtle differences to be attuned to. $o(g(n))$ contains all functions that are bounded above by $g(n)$, and not within a constant factor. For $o$ and $\\omega$, set builder notation can actually be more useful, as the differences aren't so easy to depict visually. At any rate, this is how we define $o(g(n))$ in symbols. Important characteristics of the definition have been bolded. $$o(g(n)) = \\bigg\\lbrace\\, f(n)\\, \\bigg|\\, \\textbf{for all constants}\\, c, \\,\\text{and for} \\,n \\geq n_0,\\,\\, 0\\leq f(n) < cg(n)\\bigg\\rbrace$$ This states that for ANY constant $c$, and for $n$ sufficiently large, $f(n)$ will always be strictly less than some constant multiple of $g(n)$. Though this may seem subtle, it's actually the characteristic, profound, difference between $O(g(n))$ and $o(g(n))$. This tells us that $f(n)=o(g(n))$ if and only if $f(n)$ is significantly smaller than $g(n)$, even near infinity. The limit definition is perhaps the most intuitive way to understand $o(g(n))$. If we divide the inequality in the definition above by $g(n)$, we get $$ 0\\leq \\frac{f(n)}{g(n)} < c $$ Because this holds for all values of $c$, $\\frac{f(n)}{g(n)}$ must tend to 0 as $n$ approaches infinity. In other words, $$f(n) = o(g(n))\\quad \\Leftrightarrow \\quad \\lim_{n\\to\\infty}\\frac{f(n)}{g(n)} = 0$$ Where that cute little arrow in the middle means if and only if . In broad strokes, $f(n) = o(g(n))$ tells us that $g(n)$ grows significantly faster than $f(n)$, meaning that $f(n)$ is bounded above by $g(n)$, where the bound is strict . Becuase $o(g(n))$ is a more specific criteria for upper bounds than $O(g(n))$, it follows that $o(g(n))\\subset O(g(n))$. For those unfamilar with set notation, $\\subset$ means \"subset of\". In simple terms, $A\\subset B$ means that anything in the set $A$ is also in $B$. So, if $f(n) = o(g(n))$, you can bet that $f(n) = O(g(n))$, too. If you look at the limits, or the inequality definitions, they'll check out to confirm this fact. If you're interested in a challenge, see if it works the other way around. Does $f(n) = O(g(n))$ imply $f(n) = o(g(n))$? So, $o(g(n))$ covers functions bounded above by $g(n)$, where the bound is strict. Similarly, $\\omega(g(n))$ collects all the functions that are bounded below by $g(n)$, where the lower bound is strict. This strict lower bounding has a familiar looking set-builder definition. $$\\omega(g(n)) = \\bigg\\lbrace\\, f(n)\\, \\bigg|\\, \\textbf{for all constants}\\, c, \\,\\text{and for} \\,n \\geq n_0,\\,\\, 0\\leq cg(n) < f(n)\\bigg\\rbrace$$ This time, the role of $f$ and $g$ is reversed. And, where there's functional inequalities, a limit is sure to follow. Taking the inequality from the definition: $$0 \\leq cg(n) < f(n)$$ And, dividing through by $g(n)$ $$0 \\leq c < \\frac{f(n)}{g(n)} $$ Recall that $c$, in this case, represents every possible constant. The only number strictly greater than every possible constant is the big one, infinity. So, the limiting behavior of $\\frac{f(n)}{g(n)}$ must tend to infinity. Or, more succinctly, $$f(n) = \\omega(g(n))\\quad \\Leftrightarrow \\quad \\lim_{n\\to\\infty}\\frac{f(n)}{g(n)} = \\infty$$ In Summary - $o$ and $\\omega$ define strict asymptotic bounds - $o(g(n))$ is analogous to $O(g(n))$, and defines functions bounded above by $g(n)$, where the bound is strict ( not within a constant multiple) - $\\omega(g(n))$ is analogous to $\\Omega(g(n))$, and defines functions bounded below by $g(n)$, where the bound is strict. - We've got limits for both $\\omega$ and $o$. Practice Problems {{ ADD PRACTICE PROBLEMS }}","title":"Asymptotic Function Growth"},{"location":"CSE/asym-funcs/#asymptotic-growth-of-functions","text":"[[ ADD SOME PRELIM NOTES HERE ]] For the purposes of algorithmic analysis, we're particularly concerned with how an algorithm performs when the inputs get really, really large. Two functions that sort a 10-element array at around the same speed may behave very, very differently when that 10 element array has 100, 10000, or 10000000000000 elements. As such, to explore, analyze, and classify the runtimes of various algorithms, we use (and abuse) asymptotic notation . At the heart of this exploration, we have five sets : $\\Theta$, $O$, $\\Omega$, $o$, and $\\omega$. These collections allow us to compare functions relative to their asymptotic growth rates. In other words, these sets group functions based on how they behave for sufficiently large inputs, relative to another function . [[ ADD FIGURE ]] For now, think of expressions like $f(n) = \\Omega(g(n))$ as saying 'the function $f$ is related to the function $g$ by a relationship defined by $\\Omega$' .","title":"Asymptotic Growth of Functions"},{"location":"CSE/asym-funcs/#some-things","text":"There's a couple things that we need to make clear from the onset. First, we restrict ourself to analyzing asymptotically nonnegative and asymptotically positive functions. All this means is that our functions stay positive and/or nonzero as they approach infinity. Second, the constructs $\\Theta, O, \\Omega, o, \\omega$ are sets , but we follow this strange convention of writing $f(n) = O(g(n))$ or $f(n) = \\Theta(g(n))$ to mean that $f(n)$ is in the set $O(g(n))$ or $\\Theta(g(n))$. Typically, we would use the set inclusion notation \"$\\in$\" but apparently that's out of style. (Too archaic? No ASCII symbol? I dunno.)","title":"Some Things"},{"location":"CSE/asym-funcs/#big-o","text":"Let's start with $O$ (big-oh). $O(g(n))$ is a set, a collection of all the functions $f(n)$ such that $f(n)$ is less than or equal to a constant multiple of $g(n)$, when $n$ is sufficiently large. In other words, $O(g(n))$ contains all functions bounded above by some factor of $g(n)$. Visually, this means that [[ INSERT FIGURE ]] As you can see, we're really only concerned with the behavior of functions relative to each other after a certain 'sticking point'. In the graph above, $f(n)$ does some wiggling around before it settles down to be bounded above by $c\\cdot g(n)$, and that's fine by us. On an infinite scale, it's still eventually bounded above by some multiple of $g$, and that's all that we really care about. To be formal, we can define the set $O$ explicitly, as follows In symbols: $$O(g(n)) = \\bigg\\lbrace f(n)\\,\\bigg|\\, \\exists c, n_0 \\,\\text{such that}\\, \\forall n \\geq n_0, \\,\\, 0\\leq f(n) \\leq cg(n)\\bigg\\rbrace$$ In words: The set $O(n)$ is defined as all functions $f$ such that for a choice of $c$, and a constant $n_0$, $f(n)$ is bounded above by $c\\cdot g(n)$. This states that $O(g(n))$ is the set of all functions $f(n)$ such that $f(n)$ is bounded above by a constant multiple of $g(n)$ for some constant $c$ and a 'sticking point' $n_0$. This formal definition really just re-iterates what we can see in the figure. $O(g(n))$ contains anything that can be bounded above by $g(n)$ or some constant multiple of $g(n)$, provided that we let $n$ get big enough. And, the formal definition enables us to \"prove\" that something is in $O(g(n))$, which is one of our primary goals for any of these asymptotic collections. Let's take a look at some examples, starting with a very simple one. Let $f(n) = n$ and $g(n) = n^2$. Show that $f(n) = O(g(n))$. How do we show this? Well, we know by the definition that we need to show that $0\\leq f(n)\\leq cg(n)$ for some constant $c$, and all $n \\geq n_0$. When $n=1$, we can see that $n = n^2$. After that point, $n^2 > n$. So, if we let $c=1$ and $n_0 = 1$, we find that $0\\leq f(n)\\leq cg(n)$ for all $n \\geq n_0$. As $1$ is a nonzero constant, the definition of $O$ is satisfied. And that's it! For relatively simple functions, all we need to do is find the coefficients. It's an example of constructive proof . Let's take a look at another. Prove or disprove: $3n^2+2 = O(n^2)$ > Prove or disprove: $3n^2+2 = O(n^2)$ > > In this example, the abstracted functions $f(n)$ and $g(n)$ we were working with have been replaced by their literals, $3n^2+2$ and $n^2$. > >At first, it may seem unreasonable to say that this assertion is true. How could $3n^2+2$ be bounded above by $n^2$? The secret is in the coefficients. Remember that $O(n^2)$ means that we compare functions to any constant multiple of $n^2$, not just the function itself. > > For the sake of example, look back to the definition of $O(g(n))$. As you'll see, we're entitled to **any** coefficient we want. So, it would not be \"out of bounds\" to let $c = 10,000$, for example. Clearly, $3n^2+2 \\leq 10000\\cdot n^2$ for all $n$, which proves that $3n^2+2 = O(n^2)$. However, it's helpful to illustrate a more realistic search for constants. > Let's start with what we're trying to show: > > $$0\\leq 3n^2+2 \\leq c\\cdot n^2$$ > > With a bit of re-arrangement, we get > > $$0\\leq 3 + \\frac{2}{n^2}\\leq c$$ > Now, recall that we have more or less full control over the values of $n_0$ and $c$, so long as they are constant and nonzero. For example, if we fix $n_0 = 1$, we can pretty readily determine the required coefficient to satisfy the $O(n^2)$ definition. Holding $n_0 = 1$: > > $$0\\leq 3+2 = 5 \\leq c$$ > This tells us that for all $n \\geq n_0 = 1$, and for the constant $c=5$, > > $$ 0\\leq 3n^2+2 \\leq n^2$$ > Which is precisely the definition of $3n^2+2 = O(n^2)$, completing the proof. As shown in the examples, we can prove that $f(n)=O(g(n))$ by finding a precise set of coefficients that satisfies the relation. We'll see in a little bit how we can use limits to prove these relationships without needing to find a specific example. In Summary : - Big Oh, $O(g(n))$, defines a set of functions bounded above by constant multiples of the function $g(n)$. - One way to prove that $f(n) =O(g(n))$ is to find an explicit set of coefficients that satisfy the definition: $$0\\leq f(n)\\leq c\\cdot g(n) \\quad \\forall n\\geq n_0$$","title":"Big O"},{"location":"CSE/asym-funcs/#big-omega","text":"As we just saw, $O(g(n))$ is a collection of functions bounded above by constant multiples of $g(n)$. We can explore other types of asymptotic relationships, too. Big Omega, $\\Omega(g(n))$, is another collection, this time gathering all the functions that are bounded below by constant multiples of $g(n)$. This isn't all too different from what we just saw with $O(g(n))$, except the set is just describing a different relationship. The formal definition for $\\Omega(g(n))$ is the following: $$\\Omega(g(n)) = \\bigg\\lbrace\\,f(n)\\,\\bigg|\\, \\exists c, n_0 \\,\\,\\text{such that}\\,\\, 0\\leq c\\cdot g(n) \\leq f(n) \\,\\,\\text{for all}\\,\\, n\\geq n_0\\bigg\\rbrace$$ Which, in English, means that $\\Omega(g(n))$ is a collection of all functions that are bounded below by a constant multiple of $g(n)$, for $n$ sufficiently large. (Precisely speaking, for $n$ greater than some constant $n_0$.) As before, we can visualize $\\Omega(g(n))$ with respect to some arbitrary function $f(n)$. [[ INSERT FIGURE ]] As most of this should be a re-interpretation of the concepts we covered with $O(g(n))$, let's dive right into some exercises. Provide an example here TODO Do another example here, to show that something can be both $\\Omega$ and $O(g(n))$. What the previous example shows us is that $\\Omega(g(n))$ and $O(g(n))$ aren't mutually exclusive. Interesting, right? The key here is, yet again, the way we are allowed to use constants while making asymptotic comparisons. For all practical purposes, constants are more or less irrelevant when making comparisons with respect to $\\Omega$ or $\\Theta$ of some function $g$. We'll formalize this intuition later, but if you think about what's going on, it makes a lot of sense. Even scalar multiples such as $10000\\cdot n^2$ don't make much of a difference at an infinite scale. However, when a function $f$ is related to $g$ by both $f(n)=O(g(n))$ and $f(n)=\\Omega(g(n))$, we can say even more, as we'll see in the next section. In Summary - Like $O(g(n))$, $\\Omega(g(n))$ characterizes functions that have an asymptotic relationship to $g(n)$. - Unlike $O(g(n))$, $\\Omega(g(n))$ collects all the functions that are bounded below by a constant multiple of $g(n)$. - For now, we stick to finding explicit coefficients to prove $f(n)=\\Omega(g(n))$, just as we found coefficients for $O(g(n))$.","title":"Big Omega"},{"location":"CSE/asym-funcs/#theta","text":"In the previous section, we saw that a function $f(n)$ can \"float\" between two multiples of another function $g(n)$, thereby satisfying $f=O(g(n))$ and $f=\\Omega(g(n))$. If we were to draw this on a graph for some aribitrary functions $f$ and $g$, it would look something like this: [[ INSERT FIGURE ]] As it turns out, this isn't all that uncommon. In fact, we have a special notation for these types of situations. We say that $f(n) = \\Theta(g(n))$ (Theta of g of n) when $f(n)$ is smooshed between two constant multiples of $g(n)$, for all $n$ greater than some constant $n_0$. In set-builder notation, this means: $$\\Theta(g(n)) = \\bigg\\lbrace\\,f(n)\\,\\bigg|\\,\\exists c_0, c_1, n_0 \\,\\text{such that}\\,\\, 0\\leq c_0g(n)\\leq f(n)\\leq c_1g(n)\\,\\text{for all}\\,\\, n\\geq n_0\\bigg\\rbrace$$ Notice that if $f(n)=O(g(n))$, and $f(n)=\\Omega(g(n))$, then $f(n)=\\Theta(g(n))$, by default. To see why, consider that - $f(n)=O(g(n))$ implies that for some constant $c_1$ and another constant $n_0$, $0\\leq f(n)\\leq c_1g(n)$ for $n\\geq n_0$. - $f(n)=\\Omega(g(n))$ implies that for some constant $c_0$ and another constant $n_1$, $0\\leq c_0g(n)\\leq f(n)$ for $n\\geq n_1$. - For all $n$ greater than $n_0$ or $n_1$ (whichever is bigger), both of the statements above will be true at the same time. - In symbols, this means that for $n\\geq max(n_0, n_1)$, $$0\\leq c_0g(n)\\leq f(n)\\leq c_1g(n)$$ Which is just what we wanted! $\\Theta(g(n))$ defines what is called a tight asymptotic bound. To prove $f(n)=\\Theta(g(n))$ for functions $f$ and $g$, we can do the coefficient hunting that we did before. Only this time, we need to find two coefficients. Personally, I find it a bit easier to solve $f(n)=\\Omega(g(n))$ and $f(n)=O(g(n))$ separately, and then combine the coefficients for $n$ greater than the max of the sticking point for each function. Here's an example of that idea at work. Example with $\\Omega$ and $O$ to prove $\\Theta$ TODO Internal/Dev note: play up the sticking point bit a little bit more. I think that it's instructive and intuitive. In Summary - $\\Theta, O, \\Omega$ all relate growth rates - $O(g(n))$ characterizes all functions bounded above by a factor of $g(n)$. - $\\Omega(g(n))$ characterizes all functions bounded below by a factor of $g(n)$. - $\\Theta(g(n))$ characterizes functions smooshed in between two factors of $g(n)$ - $\\bigg(f(n)=\\Omega(g(n))\\bigg) + \\bigg(f(n)=O(g(n))\\bigg) = \\bigg(f(n) = \\Theta(g(n))\\bigg)$ - For now, we're still stuck with finding coefficients to prove that this is true.","title":"$\\Theta$"},{"location":"CSE/asym-funcs/#limit-definitions","text":"Using limits to define $\\Theta, \\Omega$, and $O$ is particularly convenient. Really, we're investigating the limiting behavior (what happens at $\\infty$) of functions relative to each other, so the limit is almost a natural consequence. Take the statement $f(n)=O(g(n))$, for example. The definition of $O(g(n))$ tells us that this means $$0\\leq f(n)\\leq c\\cdot g(n)$$ For a sufficiently large $n$. However, if we divide each side by $g(n)$, we get $$0\\leq \\frac{f(n)}{g(n)} \\leq c \\qquad (1)$$ As $c$ is a constant, this inequality is equivalent to the limit $$\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = L \\quad \\text{where}\\quad 0\\leq L < \\infty \\qquad (2)$$ The limit can't be infinite, because we're locked down by the constant on the far right of the inequality. The limit, however, can be zero. If this is confusing, remember that the inequality $(1)$ must be true for all $n$. So, even as $n$ becomes infinitely large, $\\frac{f(n)}{g(n)}$ must be some finite number. We can define a similar limit for $\\Omega(g(n))$. $f(n) = \\Omega(g(n))$ implies that for sufficiently large $n$, and a constant $c$, $$0\\leq c\\cdot g(n) \\leq f(n)$$ Re-arranging as before, we can see that $$0< c\\leq \\frac{f(n)}{g(n)}$$ Which means, that as the inputs become arbitrarily large, $\\frac{f(n)}{g(n)}$ needs to stay above a certain constant threshold. Translating that inequality into a limit, we find $$\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = L \\quad \\text{where} \\quad 0 < L\\leq \\infty$$ These limits are awesome for two reasons: first, they're a bit more intuitive for comparing the relative behavior of functions as the inputs approach arbitrarily large values. Second, it makes solving complex asymptotic relationships really really easy. Here's a couple of examples to gnaw on. [[ INSERT EXAMPLES ]] $\\Theta(g(n))$ has a natural limit definition, too. The definition of $f(n) = \\Theta(g(n))$ implies that for nonzero constants $c_0, c_1$, and for $n$ beyond the sticking point $n_0$, $$0\\leq c_0g(n) \\leq f(n) \\leq c_1g(n)$$ Dividing through by $g(n)$ gives $$0 < c_0 \\leq \\frac{f(n)}{g(n)} \\leq c_1$$ Which, as a limit, implies that $$\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = L \\quad \\text{where} \\quad 0 < L < \\infty$$ Just as $\\Omega + O = \\Theta$ when we were playing around with constants, we can see that when the limiting behavior of $\\frac{f(n)}{g(n)}$ satisfies the limit definitions for both $O$ and $\\Omega$, it will naturally satisfy the limit definition for $\\Theta$. Using these limits, we can start to develop a more sophisticated insight into how functions behave relative to each other.","title":"Limit Definitions"},{"location":"CSE/asym-funcs/#littles","text":"We have two more sets to talk about before we can wrap up our exploration of asymptotic function analysis. For the purposes of comparing functions and analyzing algorithms, it can be particularly useful to classify and collection functions that aren't within a constant multiple of the function that we're comparing them to. This really isn't all that different from the things that we were doing before: the bounds just become a bit more strict. We have two collections that encode this type of information: $o$ (little-oh), which is an analog to $O$, and $\\omega$ (little-omega), which is an analog to $\\Omega$. The definitions and explanations that follow will look similar, but there's some important, subtle differences to be attuned to. $o(g(n))$ contains all functions that are bounded above by $g(n)$, and not within a constant factor. For $o$ and $\\omega$, set builder notation can actually be more useful, as the differences aren't so easy to depict visually. At any rate, this is how we define $o(g(n))$ in symbols. Important characteristics of the definition have been bolded. $$o(g(n)) = \\bigg\\lbrace\\, f(n)\\, \\bigg|\\, \\textbf{for all constants}\\, c, \\,\\text{and for} \\,n \\geq n_0,\\,\\, 0\\leq f(n) < cg(n)\\bigg\\rbrace$$ This states that for ANY constant $c$, and for $n$ sufficiently large, $f(n)$ will always be strictly less than some constant multiple of $g(n)$. Though this may seem subtle, it's actually the characteristic, profound, difference between $O(g(n))$ and $o(g(n))$. This tells us that $f(n)=o(g(n))$ if and only if $f(n)$ is significantly smaller than $g(n)$, even near infinity. The limit definition is perhaps the most intuitive way to understand $o(g(n))$. If we divide the inequality in the definition above by $g(n)$, we get $$ 0\\leq \\frac{f(n)}{g(n)} < c $$ Because this holds for all values of $c$, $\\frac{f(n)}{g(n)}$ must tend to 0 as $n$ approaches infinity. In other words, $$f(n) = o(g(n))\\quad \\Leftrightarrow \\quad \\lim_{n\\to\\infty}\\frac{f(n)}{g(n)} = 0$$ Where that cute little arrow in the middle means if and only if . In broad strokes, $f(n) = o(g(n))$ tells us that $g(n)$ grows significantly faster than $f(n)$, meaning that $f(n)$ is bounded above by $g(n)$, where the bound is strict . Becuase $o(g(n))$ is a more specific criteria for upper bounds than $O(g(n))$, it follows that $o(g(n))\\subset O(g(n))$. For those unfamilar with set notation, $\\subset$ means \"subset of\". In simple terms, $A\\subset B$ means that anything in the set $A$ is also in $B$. So, if $f(n) = o(g(n))$, you can bet that $f(n) = O(g(n))$, too. If you look at the limits, or the inequality definitions, they'll check out to confirm this fact. If you're interested in a challenge, see if it works the other way around. Does $f(n) = O(g(n))$ imply $f(n) = o(g(n))$? So, $o(g(n))$ covers functions bounded above by $g(n)$, where the bound is strict. Similarly, $\\omega(g(n))$ collects all the functions that are bounded below by $g(n)$, where the lower bound is strict. This strict lower bounding has a familiar looking set-builder definition. $$\\omega(g(n)) = \\bigg\\lbrace\\, f(n)\\, \\bigg|\\, \\textbf{for all constants}\\, c, \\,\\text{and for} \\,n \\geq n_0,\\,\\, 0\\leq cg(n) < f(n)\\bigg\\rbrace$$ This time, the role of $f$ and $g$ is reversed. And, where there's functional inequalities, a limit is sure to follow. Taking the inequality from the definition: $$0 \\leq cg(n) < f(n)$$ And, dividing through by $g(n)$ $$0 \\leq c < \\frac{f(n)}{g(n)} $$ Recall that $c$, in this case, represents every possible constant. The only number strictly greater than every possible constant is the big one, infinity. So, the limiting behavior of $\\frac{f(n)}{g(n)}$ must tend to infinity. Or, more succinctly, $$f(n) = \\omega(g(n))\\quad \\Leftrightarrow \\quad \\lim_{n\\to\\infty}\\frac{f(n)}{g(n)} = \\infty$$ In Summary - $o$ and $\\omega$ define strict asymptotic bounds - $o(g(n))$ is analogous to $O(g(n))$, and defines functions bounded above by $g(n)$, where the bound is strict ( not within a constant multiple) - $\\omega(g(n))$ is analogous to $\\Omega(g(n))$, and defines functions bounded below by $g(n)$, where the bound is strict. - We've got limits for both $\\omega$ and $o$.","title":"Littles"},{"location":"CSE/asym-funcs/#practice-problems","text":"{{ ADD PRACTICE PROBLEMS }}","title":"Practice Problems"},{"location":"CSE/cse101-f19-home/","text":"CSE101 - Fall 2019 Instructor: Patrick Tantalo Catalog Title: Introduction to Algorithms Official Text: Introduction to Algorithms 3e [CLRS] [Link to the course catalog description here] What's this course all about? Algos!!! CSE101 explores the mathematical underpinnings of algorithmic analysis, particularly asymptotic runtime analysis, recurrence relations, and deeper exploration of data structures. In the Fall '19 offering, this was accompanied by several programming assignments in C, practical applications of the theoretic components of the course. Course Prerequisites This course assumes a working understanding of the C programming language, in addition to a basic understanding of proof techniques and mathematical/computational reasoning, especially induction and recursive functions . As such, the following modules will be helpful: [Add links?] Problem Solving and Critical Thinking in Math, Computer Science, and Engineering Fundamentals of mathematical reasoning and proof Basics of C programming Induction proofs Recurrence relations Functions Series and sequences General Resources In addition to the content that's available here, the following resources have been helpful to former students. Discrete Math, An Open Introduction The C Programming Language, by K&R How to Solve It, Polya Grokking Algorithms","title":"Course Home"},{"location":"CSE/cse101-f19-home/#cse101-fall-2019","text":"Instructor: Patrick Tantalo Catalog Title: Introduction to Algorithms Official Text: Introduction to Algorithms 3e [CLRS] [Link to the course catalog description here]","title":"CSE101 - Fall 2019"},{"location":"CSE/cse101-f19-home/#whats-this-course-all-about","text":"Algos!!! CSE101 explores the mathematical underpinnings of algorithmic analysis, particularly asymptotic runtime analysis, recurrence relations, and deeper exploration of data structures. In the Fall '19 offering, this was accompanied by several programming assignments in C, practical applications of the theoretic components of the course.","title":"What's this course all about?"},{"location":"CSE/cse101-f19-home/#course-prerequisites","text":"This course assumes a working understanding of the C programming language, in addition to a basic understanding of proof techniques and mathematical/computational reasoning, especially induction and recursive functions . As such, the following modules will be helpful: [Add links?] Problem Solving and Critical Thinking in Math, Computer Science, and Engineering Fundamentals of mathematical reasoning and proof Basics of C programming Induction proofs Recurrence relations Functions Series and sequences","title":"Course Prerequisites"},{"location":"CSE/cse101-f19-home/#general-resources","text":"In addition to the content that's available here, the following resources have been helpful to former students. Discrete Math, An Open Introduction The C Programming Language, by K&R How to Solve It, Polya Grokking Algorithms","title":"General Resources"},{"location":"CSE/induction/","text":"Induction Induction is a powerful mathematical tool, and plays a key role in proving asymptotic relationships. The purpose of this article is to walk through some examples of induction applied to various problems. It ends with a proof of why induction works. The name \"mathematical induction\" is a bit of a misnomer, as there isn't any inductive reasoning applied. More often, we use induction deductively. We see a pattern in a problem, and so we assume that that pattern holds over an arbitrarily large domain. As a side note, there's some people who think that induction is a bunch of tomfoolery. If you're curious, you can point your browser here to read some more about it. In broad strokes, mathematical induction consists of a logical progression of steps: First, observe some type of pattern or consistent relationship Come up with some clever guess about a more general relationship that's occurring Or, more likely, verify an established/required relationship Prove the base case (the smallest value/level for which this relationship is true) Assume that this holds for all cases up to the $n^{th}$ This is called the inductive step or inductive hypothesis. Show that this assumption implies that the $n+1^{st}$ case is also true We can also assume up to the $n-1^{st}$ case, and then prove the $n^{th}$ If that felt a bit abstract, don't worry. Things will make more sense after a couple of examples. Induction Step-By-Step Let's start with an example that forces us to do a little bit of exploring. Playing with series is a great way to observe patterns and prove inductive results, as summations tend to be pretty regular. Consider the sum $$\\sum_{i=0}^{n} i\\cdot i!$$ What does this mean? This sum adds together all the numbers from $0$ to $n$, where each has been multiplied by its factorial. If we were to expand the first 4 terms of the series, it would look like this. $$\\begin{aligned}\\sum_{i=0}^{0} i\\cdot i! &= 0\\cdot 0! = 0 \\\\ \\sum_{i=0}^{1} i\\cdot i! &= 1\\cdot 1! + 0\\cdot 0! = 1\\cdot 1 = 1 \\\\ \\sum_{i=0}^{2} i\\cdot i! &= 2\\cdot 2! + 1\\cdot 1! + 0\\cdot 0! = 2\\cdot 2 + 1 = 5 \\\\ \\sum_{i=0}^{3} i\\cdot i! &= 3\\cdot 3! +2\\cdot 2! + 1\\cdot 1! + 0\\cdot 0! = 3\\cdot (3\\cdot2\\cdot1) + 2\\cdot 2 + 1 = 23 \\\\ \\sum_{i=0}^{4} i\\cdot i! &= 4\\cdot 4! + 3\\cdot 3! +2\\cdot 2! + 1\\cdot 1! + 0\\cdot 0! = 4\\cdot (4\\cdot3\\cdot2\\cdot1) + (3\\cdot2\\cdot1) + 2\\cdot 2 + 1 = 119 \\\\ \\end{aligned}$$ Now, it doesn't really look like there's much of a pattern here, other than the fact that the values get pretty big between each step. Why don't we take a closer look at the $3^{rd}$ iteration, to see if we can't cook up something clever. $$3\\cdot(3\\cdot2\\cdot1)+2\\cdot(2\\cdot1)+1$$ Each successive summation contains all of the terms of the previous, which means that we might be able to pull a factorial out in some way. In fact, it would be nice if we could pull out that leading three... Why don't we use the 'ol add-1, subtract-1 trick? $$3\\cdot(3\\cdot2\\cdot1)+2\\cdot(2\\cdot1)+1 + (1 - 1)$$ $$3\\cdot(3\\cdot2\\cdot1)+ 4 + 1 + 1 - 1$$ $$3\\cdot(3\\cdot2\\cdot1) + 6 - 1$$ Okay, things seem to be getting a bit neater here. What happens when we pull out that leading 3? $$3(3\\cdot2\\cdot1 + 2) - 1$$ Huh. And, we could actually pull out a two, as well. $$3\\cdot2(3\\cdot1 + 1) - 1$$ Hey now! That's lookin sharp. Simplifying yields $$3\\cdot2\\cdot4\\cdot1 - 1$$ Or perhaps more suggestively, $$4! - 1$$ Interestingly, when $n=3$, the result of the summation is $4!-1$. Could this be a pattern? Let's check with one of our other cases. When $n=4$, we got that the sum was $119$. $$(4+1)! - 1 = 5! - 1 = 120 - 1 = 119$$ Nice! It looks like we might have a pattern. Namely, $$\\sum_{i=0}^n i\\cdot i! = (n+1)! - 1$$ This is where the induction comes in. If we want to assert the truth of our prediction, we need to show that it holds for all values of $n$. Unfortunately, we can't brute force this. There's no possible ways to verify that the statement is true for all $n$ by computing each value, because we'd have an infinite number of computations. However, we have a pattern, and our input domain is positive integers. So, we can induct, and see what happens. The first step is proving the base case. For us, that is when $n=0$. We want to show that our guess is true in the smallest possible case. When $n=0$, the summation was equal to $0$, as we saw before. To verify the base case, we can just plug in our guess and see what happens. Letting $n=0$, $$(n+1)!-1 = (0+1)! - 1 = 1 - 1 = 0$$ Fantastic. The base case is proved. Now, we take the inductive step, which means that we make a pretty big assumption -- one of those \"trust me, this will work\"-types of assumptions. In formal language, we'd say something like this. Having proven the base case (where $n=1$), we assume that for all $1\\leq k < n$, $$\\sum_{i=0}^k i\\cdot i! = (k+1)! - 1$$ A couple parts of this can be tricky to understand, at first. First of all, we've introduced a new variable. What the heck is $k$ doing in there? Well, we're essentially saying that we trust our hypothesis to be true. So much so, in fact, that we just assume that it's true, for an arbitrarily large number of inputs ($0$ to $k$). What's important is what we're about to show with this hypothesis. In an induction proof, we'd probably follow the statement above with something like this: We now show that $$\\sum_{i=0}^n i\\cdot i! = (n+1)! - 1$$ The previous step is important, as it gives us the tools to prove this $n^{th}$ case. Typically, we apply induction to problems that are naturally a little bit recursive, or where each iteration builds on the previous. In our case, the crux of the proof is the fact that $$\\sum_{i=0}^n i\\cdot i! \\,=\\, n\\cdot n! + \\sum_{i=0}^{n-1} i\\cdot i!$$ Why is that important? If we look back at the inductive hypothesis that we made, we gave ourselves the assumption that the statement $$\\sum_{i=0}^k i\\cdot i! = (k+1)! - 1$$ was true, provided that $0\\leq k < n$. $n-1$ is greater than 1, and certainly less than $n$, which means that we can apply our inductive hypothesis to the second sum. In other words, by the inductive hypothesis $$\\begin{aligned} \\sum_{i=0}^n i\\cdot i! \\, &= \\, n\\cdot n! + \\sum_{i=0}^{n-1} i\\cdot i! \\\\ &= n\\cdot n! + ((n-1)+1)! - 1 \\end{aligned}$$ This is great. Now, all we need to show is that $$n\\cdot n! + ((n-1)+1)! - 1 = (n+1)! - 1$$ and we're done! From here, it's all algebra. $$\\begin{aligned} n\\cdot n! + ((n-1)+1)! - 1 &= n\\cdot n! + n! - 1 \\\\ &= n!(n+1) - 1 \\\\ &= (n+1)! - 1 \\end{aligned}$$ And, at this point, we could say something smug and clever like As desired , and end our proof. This was perhaps a long-winded introduction, but it can be helpful to walk through each step and explore what's really going on . A more terse version will be included in the example problems below to demonstrate what a typical proof might look like, smugness and all. As you walk through the problems below, I encourage you to note the 3 key characteristics of any inductive proof: The base case The inductive step Application of the inductive hypothesis to prove the result And, if you're feeling ambitious, try to solve some of these problems on your own, before proceeding to the proofs. I wish you the best of luck in your inductive wanderings! Assorted Induction Proofs [[ INSERT PRACTICE PROBLEMS BELOW WITH SOURCE CITATIONS ]] How Induction Works [[ USE THIS SPACE FOR THE THEORETICAL UNDERPINNINGS OF INDUCTION ]]","title":"Induction Primer"},{"location":"CSE/induction/#induction","text":"Induction is a powerful mathematical tool, and plays a key role in proving asymptotic relationships. The purpose of this article is to walk through some examples of induction applied to various problems. It ends with a proof of why induction works. The name \"mathematical induction\" is a bit of a misnomer, as there isn't any inductive reasoning applied. More often, we use induction deductively. We see a pattern in a problem, and so we assume that that pattern holds over an arbitrarily large domain. As a side note, there's some people who think that induction is a bunch of tomfoolery. If you're curious, you can point your browser here to read some more about it. In broad strokes, mathematical induction consists of a logical progression of steps: First, observe some type of pattern or consistent relationship Come up with some clever guess about a more general relationship that's occurring Or, more likely, verify an established/required relationship Prove the base case (the smallest value/level for which this relationship is true) Assume that this holds for all cases up to the $n^{th}$ This is called the inductive step or inductive hypothesis. Show that this assumption implies that the $n+1^{st}$ case is also true We can also assume up to the $n-1^{st}$ case, and then prove the $n^{th}$ If that felt a bit abstract, don't worry. Things will make more sense after a couple of examples.","title":"Induction"},{"location":"CSE/induction/#induction-step-by-step","text":"Let's start with an example that forces us to do a little bit of exploring. Playing with series is a great way to observe patterns and prove inductive results, as summations tend to be pretty regular. Consider the sum $$\\sum_{i=0}^{n} i\\cdot i!$$ What does this mean? This sum adds together all the numbers from $0$ to $n$, where each has been multiplied by its factorial. If we were to expand the first 4 terms of the series, it would look like this. $$\\begin{aligned}\\sum_{i=0}^{0} i\\cdot i! &= 0\\cdot 0! = 0 \\\\ \\sum_{i=0}^{1} i\\cdot i! &= 1\\cdot 1! + 0\\cdot 0! = 1\\cdot 1 = 1 \\\\ \\sum_{i=0}^{2} i\\cdot i! &= 2\\cdot 2! + 1\\cdot 1! + 0\\cdot 0! = 2\\cdot 2 + 1 = 5 \\\\ \\sum_{i=0}^{3} i\\cdot i! &= 3\\cdot 3! +2\\cdot 2! + 1\\cdot 1! + 0\\cdot 0! = 3\\cdot (3\\cdot2\\cdot1) + 2\\cdot 2 + 1 = 23 \\\\ \\sum_{i=0}^{4} i\\cdot i! &= 4\\cdot 4! + 3\\cdot 3! +2\\cdot 2! + 1\\cdot 1! + 0\\cdot 0! = 4\\cdot (4\\cdot3\\cdot2\\cdot1) + (3\\cdot2\\cdot1) + 2\\cdot 2 + 1 = 119 \\\\ \\end{aligned}$$ Now, it doesn't really look like there's much of a pattern here, other than the fact that the values get pretty big between each step. Why don't we take a closer look at the $3^{rd}$ iteration, to see if we can't cook up something clever. $$3\\cdot(3\\cdot2\\cdot1)+2\\cdot(2\\cdot1)+1$$ Each successive summation contains all of the terms of the previous, which means that we might be able to pull a factorial out in some way. In fact, it would be nice if we could pull out that leading three... Why don't we use the 'ol add-1, subtract-1 trick? $$3\\cdot(3\\cdot2\\cdot1)+2\\cdot(2\\cdot1)+1 + (1 - 1)$$ $$3\\cdot(3\\cdot2\\cdot1)+ 4 + 1 + 1 - 1$$ $$3\\cdot(3\\cdot2\\cdot1) + 6 - 1$$ Okay, things seem to be getting a bit neater here. What happens when we pull out that leading 3? $$3(3\\cdot2\\cdot1 + 2) - 1$$ Huh. And, we could actually pull out a two, as well. $$3\\cdot2(3\\cdot1 + 1) - 1$$ Hey now! That's lookin sharp. Simplifying yields $$3\\cdot2\\cdot4\\cdot1 - 1$$ Or perhaps more suggestively, $$4! - 1$$ Interestingly, when $n=3$, the result of the summation is $4!-1$. Could this be a pattern? Let's check with one of our other cases. When $n=4$, we got that the sum was $119$. $$(4+1)! - 1 = 5! - 1 = 120 - 1 = 119$$ Nice! It looks like we might have a pattern. Namely, $$\\sum_{i=0}^n i\\cdot i! = (n+1)! - 1$$ This is where the induction comes in. If we want to assert the truth of our prediction, we need to show that it holds for all values of $n$. Unfortunately, we can't brute force this. There's no possible ways to verify that the statement is true for all $n$ by computing each value, because we'd have an infinite number of computations. However, we have a pattern, and our input domain is positive integers. So, we can induct, and see what happens. The first step is proving the base case. For us, that is when $n=0$. We want to show that our guess is true in the smallest possible case. When $n=0$, the summation was equal to $0$, as we saw before. To verify the base case, we can just plug in our guess and see what happens. Letting $n=0$, $$(n+1)!-1 = (0+1)! - 1 = 1 - 1 = 0$$ Fantastic. The base case is proved. Now, we take the inductive step, which means that we make a pretty big assumption -- one of those \"trust me, this will work\"-types of assumptions. In formal language, we'd say something like this. Having proven the base case (where $n=1$), we assume that for all $1\\leq k < n$, $$\\sum_{i=0}^k i\\cdot i! = (k+1)! - 1$$ A couple parts of this can be tricky to understand, at first. First of all, we've introduced a new variable. What the heck is $k$ doing in there? Well, we're essentially saying that we trust our hypothesis to be true. So much so, in fact, that we just assume that it's true, for an arbitrarily large number of inputs ($0$ to $k$). What's important is what we're about to show with this hypothesis. In an induction proof, we'd probably follow the statement above with something like this: We now show that $$\\sum_{i=0}^n i\\cdot i! = (n+1)! - 1$$ The previous step is important, as it gives us the tools to prove this $n^{th}$ case. Typically, we apply induction to problems that are naturally a little bit recursive, or where each iteration builds on the previous. In our case, the crux of the proof is the fact that $$\\sum_{i=0}^n i\\cdot i! \\,=\\, n\\cdot n! + \\sum_{i=0}^{n-1} i\\cdot i!$$ Why is that important? If we look back at the inductive hypothesis that we made, we gave ourselves the assumption that the statement $$\\sum_{i=0}^k i\\cdot i! = (k+1)! - 1$$ was true, provided that $0\\leq k < n$. $n-1$ is greater than 1, and certainly less than $n$, which means that we can apply our inductive hypothesis to the second sum. In other words, by the inductive hypothesis $$\\begin{aligned} \\sum_{i=0}^n i\\cdot i! \\, &= \\, n\\cdot n! + \\sum_{i=0}^{n-1} i\\cdot i! \\\\ &= n\\cdot n! + ((n-1)+1)! - 1 \\end{aligned}$$ This is great. Now, all we need to show is that $$n\\cdot n! + ((n-1)+1)! - 1 = (n+1)! - 1$$ and we're done! From here, it's all algebra. $$\\begin{aligned} n\\cdot n! + ((n-1)+1)! - 1 &= n\\cdot n! + n! - 1 \\\\ &= n!(n+1) - 1 \\\\ &= (n+1)! - 1 \\end{aligned}$$ And, at this point, we could say something smug and clever like As desired , and end our proof. This was perhaps a long-winded introduction, but it can be helpful to walk through each step and explore what's really going on . A more terse version will be included in the example problems below to demonstrate what a typical proof might look like, smugness and all. As you walk through the problems below, I encourage you to note the 3 key characteristics of any inductive proof: The base case The inductive step Application of the inductive hypothesis to prove the result And, if you're feeling ambitious, try to solve some of these problems on your own, before proceeding to the proofs. I wish you the best of luck in your inductive wanderings!","title":"Induction Step-By-Step"},{"location":"CSE/induction/#assorted-induction-proofs","text":"[[ INSERT PRACTICE PROBLEMS BELOW WITH SOURCE CITATIONS ]]","title":"Assorted Induction Proofs"},{"location":"CSE/induction/#how-induction-works","text":"[[ USE THIS SPACE FOR THE THEORETICAL UNDERPINNINGS OF INDUCTION ]]","title":"How Induction Works"}]}